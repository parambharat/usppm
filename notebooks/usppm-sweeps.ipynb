{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import transformers\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import datasets\n",
    "\n",
    "from coral_pytorch.losses import coral_loss\n",
    "from coral_pytorch.dataset import levels_from_labelbatch\n",
    "from coral_pytorch.dataset import proba_to_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init and data load"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def enrich_metadata(data:pd.DataFrame)->pd.DataFrame:\n",
    "    context_mapping = {\n",
    "            \"A\": \"Human Necessities\",\n",
    "            \"B\": \"Operations and Transport\",\n",
    "            \"C\": \"Chemistry and Metallurgy\",\n",
    "            \"D\": \"Textiles\",\n",
    "            \"E\": \"Fixed Constructions\",\n",
    "            \"F\": \"Mechanical Engineering\",\n",
    "            \"G\": \"Physics\",\n",
    "            \"H\": \"Electricity\",\n",
    "            \"Y\": \"Emerging Cross-Sectional Technologies\",\n",
    "        }\n",
    "    data.loc[:, \"context_desc\"] = data[\"context\"].apply(lambda x: context_mapping[x[0]])\n",
    "    data.loc[:, \"anchor_length\"] = data[\"anchor\"].str.split().map(len)\n",
    "    data.loc[:, \"target_length\"] = data[\"target\"].str.split().map(len)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def log_col_stats(df:pd.DataFrame, col:str, stage:str):\n",
    "    stats_df = (\n",
    "        pd.DataFrame(df[col].value_counts())\n",
    "        .reset_index()\n",
    "        .rename({\"index\":col, col: \"count\"}, axis=1)\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    )\n",
    "    table = wandb.Table(dataframe=stats_df)\n",
    "    return wandb.log({f\"{stage}_{col}_counts\": table})\n",
    "    \n",
    "\n",
    "    \n",
    "def convert_df_to_dataset(data_df, stage=\"train\", problem_type=\"regression\"):\n",
    "    if stage == \"train\":\n",
    "        if problem_type == \"single_label_classification\":\n",
    "            data_df.loc[:, \"score\"] = (data_df[\"score\"] * (data_df[\"score\"].nunique()-1)).astype(int)\n",
    "            \n",
    "        elif problem_type == \"multi_label_classification\":\n",
    "            data_df.loc[:, \"score\"] = (data_df[\"score\"] * (data_df[\"score\"].nunique()-1)).astype(int)\n",
    "            data_df.loc[:, \"score\"] = pd.Series(\n",
    "                levels_from_labelbatch(data_df.score, data_df[\"score\"].nunique())\n",
    "                .numpy().tolist())\n",
    "        \n",
    "        cols = [\"anchor\", \"target\", \"context_desc\", \"score\"]\n",
    "        data_df = data_df[cols].rename({\"score\": \"label\"}, axis=1)\n",
    "    else:\n",
    "        cols = [\"anchor\", \"target\", \"context_desc\"]\n",
    "        data_df = data_df[cols]\n",
    "    data_df[\"inputs\"] = (\n",
    "        \"in the context of \" +\n",
    "        data_df[\"context_desc\"]  +\n",
    "        \" how similar is \" +\n",
    "        data_df[\"anchor\"] +\n",
    "        \" to \" +\n",
    "        data_df[\"target\"]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.Dataset.from_pandas(data_df, preserve_index=False)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_tokenizer_fn(config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name_or_path)\n",
    "\n",
    "    def tokenizer_fn(examples):\n",
    "        return tokenizer(examples[\"inputs\"])\n",
    "\n",
    "    return tokenizer, tokenizer_fn\n",
    "\n",
    "def load_and_log_datasets(config):\n",
    "    train_data = pd.read_csv(config.data_dir + \"/train.csv\").sample(frac=1, random_state=config.random_state)\n",
    "    test_data = pd.read_csv(config.data_dir +\"/test.csv\").sample(frac=1, random_state=config.random_state)\n",
    "    \n",
    "    train_data = enrich_metadata(train_data)\n",
    "    test_data = enrich_metadata(test_data)\n",
    "\n",
    "    train_table = wandb.Table(dataframe=train_data)\n",
    "    test_table = wandb.Table(dataframe=test_data)\n",
    "\n",
    "    wandb.log({\"train_dataset\": train_table, \"test_dataset\": test_table})\n",
    "    \n",
    "    for col in train_data.columns:\n",
    "        if col not in [\"id\"]:\n",
    "            log_col_stats(train_data, col, stage=\"train\")\n",
    "    \n",
    "    for col in test_data.columns:\n",
    "        if col not in [\"id\"]:\n",
    "            log_col_stats(test_data, col, stage=\"test\")\n",
    "    \n",
    "    sgkf = StratifiedGroupKFold(n_splits=int(1/config.val_size))\n",
    "    train_index, val_index = next(sgkf.split(train_data, train_data[\"score\"]*100, train_data.anchor))\n",
    "    \n",
    "    train_dataset = convert_df_to_dataset(train_data.copy(), stage=\"train\", problem_type=config.problem_type)\n",
    "    test_dataset = convert_df_to_dataset(test_data.copy(), stage=\"test\", problem_type=config.problem_type)\n",
    "\n",
    "    experiment_datasets = datasets.DatasetDict({\n",
    "        \"train\":train_dataset.select(train_index),\n",
    "        \"validation\": train_dataset.select(val_index),\n",
    "        \"test\": test_dataset}\n",
    "    )\n",
    "    \n",
    "    dropped_cols = [col for col in [\"inputs\", \"anchor\", \"target\", \"context_desc\", \"label\"] if not col == \"label\"]\n",
    "    tokenizer, tokenizer_fn = load_tokenizer_fn(config)\n",
    "    processed_datasets = experiment_datasets.map(\n",
    "        tokenizer_fn, \n",
    "        batched=True,\n",
    "        remove_columns=dropped_cols,\n",
    "        num_proc=10,\n",
    "    )\n",
    "    return processed_datasets, tokenizer\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pearson_corr_reg(eval_pred):\n",
    "    predictions = eval_pred.predictions.flatten()\n",
    "    labels = eval_pred.label_ids\n",
    "    return {'pearson': np.corrcoef(labels, predictions)[0][1]}\n",
    "\n",
    "def pearson_corr_clf(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1).flatten().astype(float)\n",
    "    labels = eval_pred.label_ids.astype(float)    \n",
    "    return {'pearson': np.corrcoef(predictions, labels)[0][1]}\n",
    "\n",
    "def pearson_corr_ord(eval_pred):\n",
    "    predictions = proba_to_label(torch.Tensor(expit(eval_pred.predictions))).numpy()\n",
    "    labels = eval_pred.label_ids\n",
    "    labels = (labels==0).argmax(axis=1)\n",
    "    return {'pearson': np.corrcoef(predictions, labels)[0][1]}\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CoralTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = coral_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def get_trainer(config,tokenizer,dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        config.output_dir,\n",
    "        learning_rate=config.learning_rate, \n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size,\n",
    "        num_train_epochs=config.epochs,\n",
    "        weight_decay=config.weight_decay,\n",
    "        report_to=\"wandb\",\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        \n",
    "    )\n",
    "\n",
    "    if config.problem_type == \"regression\":\n",
    "        config.num_labels = 1\n",
    "        metric_fn = pearson_corr_reg\n",
    "\n",
    "    elif config.problem_type == \"single_label_classification\":\n",
    "        config.num_labels = 5\n",
    "        metric_fn = pearson_corr_clf\n",
    "\n",
    "    elif config.problem_type == \"multi_label_classification\":\n",
    "        config.num_labels = 4\n",
    "        metric_fn = pearson_corr_ord\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.model_name_or_path,\n",
    "        num_labels=config.num_labels,\n",
    "        problem_type=config.problem_type\n",
    "    )\n",
    "    if config.problem_type == \"multi_label_classification\":\n",
    "        trainer = CoralTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset=dataset['validation'],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=metric_fn,)\n",
    "    else:\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset=dataset['validation'],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=metric_fn,)\n",
    "    return trainer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "defaults = {\n",
    "  \"batch_size\": 64,\n",
    "  \"data_dir\": \"../data\",\n",
    "  \"entity\": \"parambharat\",\n",
    "  \"epochs\": 5,\n",
    "  \"learning_rate\": 0.000003,\n",
    "  \"log_dir\": \"../logs\",\n",
    "  \"lr_scheduler_type\": \"cosine\",\n",
    "  \"model_name_or_path\": \"anferico/bert-for-patents\",\n",
    "  \"output_dir\": \"../outputs\",\n",
    "  \"problem_type\": \"regression\",\n",
    "  \"project_name\": \"usppm\",\n",
    "  \"random_state\": 42,\n",
    "  \"val_size\": 0.25,\n",
    "  \"warmup_ratio\": 0.1,\n",
    "  \"weight_decay\": 0.01\n",
    "}\n",
    "\n",
    "sweep_config={\n",
    "  \"method\": \"random\",\n",
    "  \"metric\": {\n",
    "    \"goal\": \"maximize\",\n",
    "    \"name\": \"pearson\"\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"batch_size\": {\n",
    "      \"distribution\": \"q_log_uniform_values\",\n",
    "      \"max\": 256,\n",
    "      \"min\": 32,\n",
    "      \"q\": 8\n",
    "    },\n",
    "    \"data_dir\": {\n",
    "      \"value\": \"../data\"\n",
    "    },\n",
    "    \"dropout\": {\n",
    "      \"values\": [\n",
    "        0.3,\n",
    "        0.4,\n",
    "        0.5\n",
    "      ]\n",
    "    },\n",
    "    \"epochs\": {\n",
    "      \"values\": [\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "        7\n",
    "      ]\n",
    "    },\n",
    "    \"learning_rate\": {\n",
    "      \"distribution\": \"uniform\",\n",
    "      \"max\": 0.001,\n",
    "      \"min\": 0.000005\n",
    "    },\n",
    "    \"log_dir\": {\n",
    "      \"value\": \"../logs\"\n",
    "    },\n",
    "    \"model_name_or_path\": {\n",
    "      \"values\": [\n",
    "        \"microsoft/deberta-v3-small\",\n",
    "        \"AI-Growth-Lab/PatentSBERTa\",\n",
    "        \"anferico/bert-for-patents\",\n",
    "        \"distilbert-base-uncased\",\n",
    "        \"distilroberta-base\"\n",
    "      ]\n",
    "    },\n",
    "    \"output_dir\": {\n",
    "      \"value\": \"../outputs\"\n",
    "    },\n",
    "    \"problem_type\": {\n",
    "      \"values\": [\n",
    "        \"regression\",\n",
    "        \"single_label_classification\",\n",
    "        \"multi_label_classification\"\n",
    "      ]\n",
    "    },\n",
    "    \"random_state\": {\n",
    "      \"value\": 42\n",
    "    },\n",
    "    \"val_size\": {\n",
    "      \"value\": 0.25\n",
    "    },\n",
    "    \"warmup_ratio\": {\n",
    "      \"value\": 0.1\n",
    "    },\n",
    "    \"weight_decay\": {\n",
    "      \"values\": [\n",
    "        0.01,\n",
    "        0.03,\n",
    "        0.001,\n",
    "        0.003\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=defaults[\"project_name\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_fn(config=defaults):\n",
    "    with wandb.init(project=defaults[\"project_name\"], config=config) as run:\n",
    "        config = run.config\n",
    "        dds, tokenizer = load_and_log_datasets(config)\n",
    "        trainer = get_trainer(config,tokenizer,dds)\n",
    "        trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train_fn, count=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "# trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = test_dataset.map(\n",
    "    tokenizer_fn, \n",
    "    batched=True,\n",
    "    remove_columns=dropped_cols,\n",
    "    num_proc=10,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config.problem_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config.problem_type = config[\"problem_type\"] if not run.config.problem_type else run.config.problem_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, problem_type=\"regression\"):\n",
    "    model_outputs = trainer.predict(dataset)\n",
    "    if problem_type == \"regression\":\n",
    "        predictions = model_outputs.predictions.flatten().clip(0,1)\n",
    "    elif problem_type == \"single_label_classification\":\n",
    "        predictions = np.argmax(model_outputs.predictions, axis=1)\n",
    "        predictions = predictions / (run.config.num_labels -1)\n",
    "    elif problem_type == \"multi_label_classification\":\n",
    "        predictions = (expit(model_outputs.predictions) > 0.5).astype(int)\n",
    "        predictions = expit(model_outputs.predictions)\n",
    "#         predictions = (predictions==0).argmax(axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(inference_dataset, problem_type=run.config.problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions = predict(inference_dataset, problem_type=run.config.problem_type)\n",
    "# val_predictions = predict(preprocessed_datasets[\"validation\"], problem_type=run.config.problem_type)\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = train_data.iloc[val_index]\n",
    "\n",
    "val_data.loc[:, \"prediction\"] = val_predictions\n",
    "val_table = wandb.Table(dataframe=val_data)\n",
    "run.log({\"val_predictions\": val_table})\n",
    "\n",
    "\n",
    "test_data.loc[:, \"prediction\"] = test_predictions\n",
    "test_table = wandb.Table(dataframe=test_data)\n",
    "\n",
    "run.log({\"test_predictions\": test_table})\n",
    "\n",
    "\n",
    "submissions_df = test_data[[\"id\", 'prediction']].rename({\"prediction\": \"score\"}, axis=1)\n",
    "\n",
    "submission_fname = run.config.output_dir+\"/submission_\"+wandb.util.generate_id()+\".csv\"\n",
    "submissions_df.to_csv(submission_fname, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}